#!/usr/bin/env python3
"""
Test script for gpt-oss-safeguard model via OpenRouter API.
Tests spam detection policy with binary output format.
"""

import os
import json
import time
import argparse
import csv
from datetime import datetime
from pathlib import Path
from typing import List, Dict
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Create logs directory
LOGS_DIR = Path("logs")
LOGS_DIR.mkdir(exist_ok=True)


def load_policy(category: str, policy_name: str = "policy.txt") -> str:
    """Load policy from text file in policies/{category}/ folder."""
    policy_path = Path("policies") / category / policy_name
    if not policy_path.exists():
        raise FileNotFoundError(
            f"Policy file not found: {policy_path}\n"
            f"Please ensure the policy file exists at: {policy_path.absolute()}"
        )
    return policy_path.read_text(encoding='utf-8')


def load_test_cases(category: str, dataset_name: str = "tests.csv") -> List[Dict]:
    """Load test cases from CSV in datasets/{category}/ folder."""
    csv_path = Path("datasets") / category / dataset_name
    if not csv_path.exists():
        raise FileNotFoundError(
            f"Dataset file not found: {csv_path}\n"
            f"Please ensure the CSV file exists at: {csv_path.absolute()}"
        )

    test_cases = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for idx, row in enumerate(reader, 1):
            # Skip empty rows
            if not any(row.values()):
                continue

            # Get test name (use test_name column if available, otherwise auto-generate)
            test_name = row.get("test_name", "").strip()
            if not test_name:
                test_name = f"Test {idx}"

            # Get test content (try multiple column name variations)
            test_content = (
                row.get("test_content", "") or
                row.get("content", "") or
                row.get("text", "")
            ).strip()

            if not test_content:
                continue  # Skip rows without content

            # Get classification and convert SP levels to VALID/INVALID
            classification = (
                row.get("gpt-oss-safeguard classification", "") or
                row.get("classification", "") or
                row.get("expected", "")
            ).strip()

            # Map severity levels to VALID/INVALID
            # Level 0 (any category: SP0, HS0, VL0, etc.) = VALID
            # Level 2, 3, 4 = INVALID
            classification_upper = classification.upper()

            if classification_upper.endswith("0"):
                # Any level 0 is VALID
                expected = "VALID"
            elif any(classification_upper.endswith(level) for level in ["2", "3", "4"]):
                # Any level 2, 3, or 4 is INVALID
                expected = "INVALID"
            elif classification_upper in ["VALID", "INVALID"]:
                # Already in correct format
                expected = classification_upper
            else:
                # Default to INVALID if unclear
                expected = "INVALID"

            test_cases.append({
                "name": test_name,
                "content": test_content,
                "expected": expected,
                "sp_level": classification  # Keep original SP level for reference
            })

    if not test_cases:
        raise ValueError(
            f"No valid test cases found in {csv_path}\n"
            f"Please ensure the CSV has columns: test_name, test_content, and classification"
        )

    return test_cases


def list_categories() -> List[str]:
    """Discover all available test categories from policies folder structure."""
    policies_dir = Path("policies")
    if not policies_dir.exists():
        return []
    return [d.name for d in policies_dir.iterdir() if d.is_dir()]


def log_to_jsonl(log_entry: dict):
    """Append a log entry to the JSONL file."""
    with open(LOG_FILE, 'a') as f:
        f.write(json.dumps(log_entry) + '\n')


def log_debug(debug_entry: dict):
    """Append a debug entry to the debug JSONL file."""
    with open(DEBUG_LOG_FILE, 'a') as f:
        f.write(json.dumps(debug_entry, default=str) + '\n')


def calculate_cost(prompt_tokens: int, completion_tokens: int) -> float:
    """Calculate cost based on OpenRouter pricing for gpt-oss-safeguard-20b.

    Pricing: $0.075/M input tokens, $0.30/M output tokens
    """
    input_cost = (prompt_tokens / 1_000_000) * 0.075
    output_cost = (completion_tokens / 1_000_000) * 0.30
    return input_cost + output_cost


def validate_reasoning(reasoning_text: str, classification: str, category: str) -> Dict:
    """Validate reasoning quality based on several criteria.

    This function checks if the model's reasoning:
    1. Exists and is substantial
    2. References policy concepts
    3. Considers severity levels
    4. Addresses the specific category

    Args:
        reasoning_text: The reasoning output from the model
        classification: The classification result (VALID/INVALID)
        category: The policy category being tested

    Returns:
        Dict with validation metrics:
        - has_reasoning: bool - Whether reasoning exists
        - reasoning_length: int - Word count
        - mentions_policy: bool - References policy sections
        - mentions_severity: bool - Discusses severity levels
        - mentions_category: bool - References category-specific terms
        - quality_score: float - Overall quality score 0-10
    """
    if not reasoning_text:
        return {
            "has_reasoning": False,
            "reasoning_length": 0,
            "mentions_policy": False,
            "mentions_severity": False,
            "mentions_category": False,
            "quality_score": 0.0
        }

    # Convert to lowercase for case-insensitive matching
    reasoning_lower = reasoning_text.lower()

    # Check length
    word_count = len(reasoning_text.split())
    has_substantial_reasoning = word_count >= 10

    # Check for policy references
    policy_keywords = ["policy", "rule", "criteria", "definition", "section", "category", "level"]
    mentions_policy = any(keyword in reasoning_lower for keyword in policy_keywords)

    # Check for severity level discussion
    severity_keywords = ["severity", "level", "0", "2", "3", "4", "serious", "extreme", "moderate", "low confidence", "high confidence", "strong confidence"]
    mentions_severity = any(keyword in reasoning_lower for keyword in severity_keywords)

    # Check for category-specific terms based on category
    category_terms_map = {
        "spam": ["spam", "promotional", "solicitation", "bulk", "phishing", "scam", "unsolicited"],
        "hate-speech": ["hate", "harassment", "discrimination", "protected", "dehumaniz", "slur", "stereotype"],
        "violence": ["violence", "threat", "harm", "attack", "weapon", "injury", "death"],
        "sexual-content": ["sexual", "explicit", "csam", "minor", "consent", "solicitation"],
        "self-harm": ["self-harm", "suicide", "depression", "eating disorder", "cutting", "crisis"],
        "fraud": ["fraud", "phishing", "scam", "deceptive", "identity theft", "financial"],
        "illegal-activity": ["illegal", "drug", "trafficking", "weapon", "criminal", "prohibited"]
    }

    category_terms = category_terms_map.get(category, [])
    mentions_category = any(term in reasoning_lower for term in category_terms)

    # Calculate quality score (0-10)
    score = 0.0
    if has_substantial_reasoning:
        score += 3.0  # Base score for having reasoning
    if mentions_policy:
        score += 2.5  # References policy framework
    if mentions_severity:
        score += 2.5  # Discusses severity
    if mentions_category:
        score += 2.0  # Category-specific analysis

    # Bonus for detailed reasoning (>30 words)
    if word_count > 30:
        score = min(10.0, score + 1.0)

    return {
        "has_reasoning": True,
        "reasoning_length": word_count,
        "mentions_policy": mentions_policy,
        "mentions_severity": mentions_severity,
        "mentions_category": mentions_category,
        "quality_score": round(score, 1)
    }


def test_safeguard(
    category: str,
    model_name: str = "openai/gpt-oss-safeguard-20b",
    debug_mode: bool = False,
    test_number: int = None,
    policy_name: str = "policy.txt",
    dataset_name: str = "tests.csv"
):
    """
    Test the gpt-oss-safeguard model with policy and test cases loaded from files.

    Args:
        category: Policy category to test (e.g., 'spam', 'nsfw', 'fraud')
        model_name: Name of the OpenRouter model to use
        debug_mode: Enable verbose debug logging with raw API responses
        test_number: Run only a specific test. If None, runs all tests.
        policy_name: Policy filename within the category folder (default: policy.txt)
        dataset_name: Dataset CSV filename within the category folder (default: tests.csv)
    """
    # Load policy and test cases from files
    print(f"\nLoading policy and test cases for category: {category}")
    policy = load_policy(category, policy_name)
    test_cases = load_test_cases(category, dataset_name)
    print(f"Loaded {len(test_cases)} test cases from {dataset_name}\n")

    # Setup log files with category name
    global LOG_FILE, DEBUG_LOG_FILE
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    LOG_FILE = LOGS_DIR / f"safeguard_test_{category}_{timestamp}.jsonl"
    DEBUG_LOG_FILE = LOGS_DIR / f"safeguard_debug_{category}_{timestamp}.jsonl"

    # Initialize OpenRouter client
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        raise ValueError("OPENROUTER_API_KEY not found in environment variables")

    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=api_key,
    )

    # Handle single test selection
    tests_to_run = test_cases
    if test_number is not None:
        if test_number < 1 or test_number > len(test_cases):
            raise ValueError(f"Test number must be between 1 and {len(test_cases)}")
        tests_to_run = [test_cases[test_number - 1]]
        print(f"\n{'='*80}")
        print(f"DEBUG MODE: Running single test #{test_number}")
        print(f"{'='*80}\n")

    print(f"Testing {model_name} with {category} policy via OpenRouter\n")
    print(f"Logging to: {LOG_FILE}")
    if debug_mode:
        print(f"Debug log: {DEBUG_LOG_FILE}")
    print("\n" + "=" * 80)

    # Log session start
    session_start = {
        "event_type": "session_start",
        "timestamp": datetime.now().isoformat(),
        "model": model_name,
        "total_tests": len(tests_to_run),
        "debug_mode": debug_mode,
        "single_test": test_number,
        "log_file": str(LOG_FILE)
    }
    log_to_jsonl(session_start)

    if debug_mode:
        log_debug({
            "event_type": "debug_session_start",
            "timestamp": datetime.now().isoformat(),
            "debug_log_file": str(DEBUG_LOG_FILE),
            "mode": "single_test" if test_number else "all_tests"
        })

    results = []
    total_tokens = {"prompt": 0, "completion": 0, "total": 0}
    total_cost = 0.0
    total_latency = 0.0
    reasoning_scores = []

    for idx, test_case in enumerate(tests_to_run, 1):
        # Calculate actual test number (for display purposes)
        i = test_number if test_number else idx
        print(f"\nTest {i}/{len(test_cases)}: {test_case['name']}")
        print(f"Content: {test_case['content']}")
        print(f"Expected: {test_case['expected']}")

        request_timestamp = datetime.now().isoformat()
        start_time = time.time()

        try:
            # Prepare request
            request_payload = {
                "model": model_name,
                "messages": [
                    {
                        "role": "system",
                        "content": policy
                    },
                    {
                        "role": "user",
                        "content": test_case['content']
                    }
                ]
            }

            if debug_mode:
                print("\n" + "=" * 80)
                print("DEBUG: REQUEST PAYLOAD")
                print("=" * 80)
                print(json.dumps(request_payload, indent=2))
                print("=" * 80 + "\n")

            # Call OpenRouter with the policy as system message and content as user message
            response = client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://raxit.ai/",
                    "X-Title": "raxIT AI",
                },
                **request_payload
            )

            # Calculate latency
            latency_ms = (time.time() - start_time) * 1000

            # Convert response to dict for complete inspection
            response_dict = response.model_dump() if hasattr(response, 'model_dump') else response.dict()

            if debug_mode:
                print("\n" + "=" * 80)
                print("DEBUG: RAW API RESPONSE")
                print("=" * 80)
                print(json.dumps(response_dict, indent=2, default=str))
                print("=" * 80 + "\n")

            # Extract the model's response
            result = response.choices[0].message.content.strip()

            # Extract reasoning if available
            message = response.choices[0].message
            reasoning = getattr(message, 'reasoning', None)

            # Validate reasoning quality
            reasoning_validation = validate_reasoning(reasoning or "", result, category)

            # Get token usage
            usage = response.usage
            prompt_tokens = usage.prompt_tokens if usage else 0
            completion_tokens = usage.completion_tokens if usage else 0
            total_tokens_used = usage.total_tokens if usage else 0

            # Calculate cost
            cost = calculate_cost(prompt_tokens, completion_tokens)

            # Update totals
            total_tokens["prompt"] += prompt_tokens
            total_tokens["completion"] += completion_tokens
            total_tokens["total"] += total_tokens_used
            total_cost += cost
            total_latency += latency_ms

            # Check if result matches expected
            is_correct = result == test_case['expected']

            print(f"Model Output: {result}")
            print(f"Tokens: {prompt_tokens} prompt / {completion_tokens} completion / {total_tokens_used} total")
            print(f"Cost: ${cost:.6f} | Latency: {latency_ms:.0f}ms")
            print(f"Reasoning Quality: {reasoning_validation['quality_score']}/10 ({reasoning_validation['reasoning_length']} words)")
            print(f"Status: {'✓ PASS' if is_correct else '✗ FAIL'}")

            # Log detailed request/response
            log_entry = {
                "event_type": "inference",
                "timestamp": request_timestamp,
                "test_number": i,
                "test_name": test_case['name'],
                "request": {
                    "model": model_name,
                    "messages": [
                        {"role": "system", "content": policy[:200] + "..."},  # Truncate policy for readability
                        {"role": "user", "content": test_case['content']}
                    ]
                },
                "response": {
                    "id": response.id if hasattr(response, 'id') else None,
                    "model": response.model if hasattr(response, 'model') else model_name,
                    "content": result,
                    "finish_reason": response.choices[0].finish_reason if response.choices else None
                },
                "usage": {
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": total_tokens_used
                },
                "metrics": {
                    "latency_ms": round(latency_ms, 2),
                    "cost_usd": round(cost, 8)
                },
                "test_result": {
                    "expected": test_case['expected'],
                    "actual": result,
                    "passed": is_correct
                }
            }

            # Add reasoning if available (just the text)
            if reasoning:
                log_entry["reasoning"] = reasoning
                log_entry["reasoning_validation"] = reasoning_validation
            log_to_jsonl(log_entry)

            # Log complete debug information if debug mode is enabled
            if debug_mode:
                debug_entry = {
                    "event_type": "debug_inference",
                    "timestamp": request_timestamp,
                    "test_number": i,
                    "test_name": test_case['name'],
                    "request": {
                        "model": model_name,
                        "messages": request_payload["messages"],  # Full untruncated policy
                        "extra_headers": {
                            "HTTP-Referer": "https://raxit.ai/",
                            "X-Title": "raxIT AI",
                        }
                    },
                    "response_raw": response_dict,  # Complete raw response object
                    "response_extracted": {
                        "content": result,
                        "finish_reason": response.choices[0].finish_reason if response.choices else None,
                        "model": response.model if hasattr(response, 'model') else model_name,
                        "id": response.id if hasattr(response, 'id') else None,
                    },
                    "usage": {
                        "prompt_tokens": prompt_tokens,
                        "completion_tokens": completion_tokens,
                        "total_tokens": total_tokens_used
                    },
                    "metrics": {
                        "latency_ms": round(latency_ms, 2),
                        "cost_usd": round(cost, 8)
                    },
                    "test_result": {
                        "expected": test_case['expected'],
                        "actual": result,
                        "passed": is_correct
                    }
                }
                log_debug(debug_entry)

                print("\n" + "=" * 80)
                print("DEBUG: Detailed entry written to debug log")
                print("=" * 80)

            # Track reasoning scores
            reasoning_scores.append(reasoning_validation['quality_score'])

            results.append({
                "test_name": test_case['name'],
                "expected": test_case['expected'],
                "actual": result,
                "correct": is_correct,
                "reasoning_quality": reasoning_validation['quality_score']
            })

        except Exception as e:
            latency_ms = (time.time() - start_time) * 1000
            error_msg = str(e)

            print(f"Error: {error_msg}")

            # Log error
            log_entry = {
                "event_type": "error",
                "timestamp": request_timestamp,
                "test_number": i,
                "test_name": test_case['name'],
                "error": error_msg,
                "latency_ms": round(latency_ms, 2)
            }
            log_to_jsonl(log_entry)

            results.append({
                "test_name": test_case['name'],
                "expected": test_case['expected'],
                "actual": f"ERROR: {error_msg}",
                "correct": False
            })

        print("-" * 80)

    # Summary
    print("\n" + "=" * 80)
    print("SUMMARY")
    print("=" * 80)

    total = len(results)
    passed = sum(1 for r in results if r['correct'])
    accuracy = (passed / total) * 100 if total > 0 else 0
    avg_latency = total_latency / total if total > 0 else 0

    # Calculate reasoning metrics
    avg_reasoning_quality = sum(reasoning_scores) / len(reasoning_scores) if reasoning_scores else 0
    reasoning_presence_rate = (len([s for s in reasoning_scores if s > 0]) / len(reasoning_scores) * 100) if reasoning_scores else 0

    print(f"Total Tests: {total}")
    print(f"Passed: {passed}")
    print(f"Failed: {total - passed}")
    print(f"Accuracy: {accuracy:.1f}%")
    print(f"\nReasoning Quality:")
    print(f"  Average Quality Score: {avg_reasoning_quality:.1f}/10")
    print(f"  Reasoning Presence Rate: {reasoning_presence_rate:.1f}%")
    print(f"\nToken Usage:")
    print(f"  Prompt Tokens: {total_tokens['prompt']:,}")
    print(f"  Completion Tokens: {total_tokens['completion']:,}")
    print(f"  Total Tokens: {total_tokens['total']:,}")
    print(f"\nCost & Performance:")
    print(f"  Total Cost: ${total_cost:.6f}")
    print(f"  Average Latency: {avg_latency:.0f}ms")
    print(f"\nLog File: {LOG_FILE}")

    # Show failures
    failures = [r for r in results if not r['correct']]
    if failures:
        print("\nFailed Tests:")
        for failure in failures:
            print(f"  - {failure['test_name']}: Expected '{failure['expected']}', Got '{failure['actual']}'")

    # Log session summary
    session_summary = {
        "event_type": "session_summary",
        "timestamp": datetime.now().isoformat(),
        "model": model_name,
        "results": {
            "total_tests": total,
            "passed": passed,
            "failed": total - passed,
            "accuracy_percent": round(accuracy, 2)
        },
        "reasoning_metrics": {
            "average_quality_score": round(avg_reasoning_quality, 2),
            "reasoning_presence_rate_percent": round(reasoning_presence_rate, 2),
            "total_with_reasoning": len([s for s in reasoning_scores if s > 0])
        },
        "usage": {
            "total_prompt_tokens": total_tokens['prompt'],
            "total_completion_tokens": total_tokens['completion'],
            "total_tokens": total_tokens['total']
        },
        "metrics": {
            "total_cost_usd": round(total_cost, 8),
            "average_latency_ms": round(avg_latency, 2),
            "total_latency_ms": round(total_latency, 2)
        },
        "failures": [
            {
                "test_name": f['test_name'],
                "expected": f['expected'],
                "actual": f['actual']
            }
            for f in failures
        ] if failures else []
    }
    log_to_jsonl(session_summary)

    return results


if __name__ == "__main__":
    # Get available categories for help text
    available_categories = list_categories()
    categories_text = ", ".join(available_categories) if available_categories else "none (create folders in policies/)"

    parser = argparse.ArgumentParser(
        description="Test gpt-oss-safeguard model with policy loaded from files",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
Examples:
  # Run all spam tests
  uv run test_safeguard.py spam

  # Run test #4 in debug mode with verbose output
  uv run test_safeguard.py spam --debug --test-number 4

  # Use a different dataset file (e.g., old golden_dataset.csv)
  uv run test_safeguard.py spam --dataset-name golden_dataset.csv

  # Use a different policy file
  uv run test_safeguard.py spam --policy-name policy_v2.txt

Available categories: {categories_text}
        """
    )

    parser.add_argument(
        "category",
        type=str,
        help="Policy category to test (e.g., spam, nsfw, fraud)"
    )

    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug mode with verbose logging and raw API response dumps"
    )

    parser.add_argument(
        "--test-number",
        type=int,
        metavar="N",
        help="Run only a specific test number. Use with --debug for detailed inspection."
    )

    parser.add_argument(
        "--model",
        type=str,
        default="openai/gpt-oss-safeguard-20b",
        help="OpenRouter model name (default: openai/gpt-oss-safeguard-20b)"
    )

    parser.add_argument(
        "--policy-name",
        type=str,
        default="policy.txt",
        help="Policy filename within the category folder (default: policy.txt)"
    )

    parser.add_argument(
        "--dataset-name",
        type=str,
        default="tests.csv",
        help="Dataset CSV filename within the category folder (default: tests.csv)"
    )

    args = parser.parse_args()

    test_safeguard(
        category=args.category,
        model_name=args.model,
        debug_mode=args.debug,
        test_number=args.test_number,
        policy_name=args.policy_name,
        dataset_name=args.dataset_name
    )
