{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Dataset Exploration\n",
    "\n",
    "**Dataset:** ucberkeley-dlab/measuring-hate-speech\n",
    "\n",
    "This notebook explores the hate speech dataset to find examples for filling placeholders:\n",
    "- `[group]` - Target identity groups\n",
    "- `[stereotype]` - Stereotypical statements\n",
    "- `[slur]` - Actual slurs and offensive language\n",
    "- `[ethnic group]` - Ethnic/racial group references\n",
    "- `[industry]` - Industry/occupation stereotypes\n",
    "- `[list of actual slurs used in study]` - Collection of slurs from research context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!uv pip install datasets huggingface_hub pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace (if needed)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# This dataset is public, but login may help with rate limits\n",
    "# Uncomment if you want to login:\n",
    "# login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ucberkeley-dlab/measuring-hate-speech dataset...\n",
      "This may take a few minutes on first load (will cache for future use)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648b306077664043873b8dd7e5d553fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b818be423d416780e293414335ca62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "measuring-hate-speech.parquet:   0%|          | 0.00/14.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c776549dc6c04e32b82e951e8e971483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/135556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded successfully!\n",
      "Total examples: 135,556\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Loading ucberkeley-dlab/measuring-hate-speech dataset...\")\n",
    "print(\"This may take a few minutes on first load (will cache for future use)\\n\")\n",
    "\n",
    "ds = load_dataset(\"ucberkeley-dlab/measuring-hate-speech\")\n",
    "data = ds['train']\n",
    "\n",
    "print(f\"✓ Dataset loaded successfully!\")\n",
    "print(f\"Total examples: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET SCHEMA\n",
      "================================================================================\n",
      "\n",
      "Total columns: 131\n",
      "\n",
      "Key columns:\n",
      "  - text\n",
      "  - hate_speech_score\n",
      "  - insult\n",
      "  - dehumanize\n",
      "  - violence\n",
      "\n",
      "Target identity columns (53):\n",
      "  - target_race_asian\n",
      "  - target_race_black\n",
      "  - target_race_latinx\n",
      "  - target_race_middle_eastern\n",
      "  - target_race_native_american\n",
      "  - target_race_pacific_islander\n",
      "  - target_race_white\n",
      "  - target_race_other\n",
      "  - target_race\n",
      "  - target_religion_atheist\n",
      "  - target_religion_buddhist\n",
      "  - target_religion_christian\n",
      "  - target_religion_hindu\n",
      "  - target_religion_jewish\n",
      "  - target_religion_mormon\n",
      "  ... and 38 more\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATASET SCHEMA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal columns: {len(data.column_names)}\")\n",
    "print(\"\\nKey columns:\")\n",
    "for col in ['text', 'hate_speech_score', 'insult', 'dehumanize', 'violence']:\n",
    "    if col in data.column_names:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# Show target group columns\n",
    "target_cols = [col for col in data.column_names if col.startswith('target_')]\n",
    "print(f\"\\nTarget identity columns ({len(target_cols)}):\")\n",
    "for col in target_cols[:15]:\n",
    "    print(f\"  - {col}\")\n",
    "if len(target_cols) > 15:\n",
    "    print(f\"  ... and {len(target_cols) - 15} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "Text: Yes indeed. She sort of reminds me of the elder lady that played the part in the movie \"Titanic\" who was telling her story!!! And I wouldn't have wanted to cover who I really am!! I would be proud!!!!...\n",
      "Hate speech score: -3.9\n",
      "Insult: 0.0\n",
      "Dehumanize: 0.0\n",
      "Target groups: race_asian, race_black, race_latinx, race_middle_eastern, race_native_american, race_pacific_islander, race_white, race\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SAMPLE EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show first example in detail\n",
    "first = data[0]\n",
    "print(f\"\\nExample 1:\")\n",
    "print(f\"Text: {first['text'][:200]}...\" if len(first['text']) > 200 else f\"Text: {first['text']}\")\n",
    "print(f\"Hate speech score: {first['hate_speech_score']}\")\n",
    "print(f\"Insult: {first['insult']}\")\n",
    "print(f\"Dehumanize: {first['dehumanize']}\")\n",
    "\n",
    "# Show which groups are targeted\n",
    "targeted_groups = [col.replace('target_', '') for col in target_cols if first.get(col) == True]\n",
    "if targeted_groups:\n",
    "    print(f\"Target groups: {', '.join(targeted_groups)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Hate Speech Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HATE SPEECH SCORE DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Min score: -8.34\n",
      "Max score: 6.30\n",
      "Mean score: -0.57\n",
      "\n",
      "By severity:\n",
      "  High hate (>2.0): 20,338\n",
      "  Moderate (1.0-2.0): 20,382\n",
      "  Mild (0.5-1.0): 8,553\n",
      "  Low/neutral (<0.5): 86,283\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"HATE SPEECH SCORE DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "scores = [ex['hate_speech_score'] for ex in data if ex['hate_speech_score'] is not None]\n",
    "\n",
    "print(f\"\\nMin score: {min(scores):.2f}\")\n",
    "print(f\"Max score: {max(scores):.2f}\")\n",
    "print(f\"Mean score: {sum(scores)/len(scores):.2f}\")\n",
    "\n",
    "# Count by severity\n",
    "print(\"\\nBy severity:\")\n",
    "print(f\"  High hate (>2.0): {len([s for s in scores if s > 2.0]):,}\")\n",
    "print(f\"  Moderate (1.0-2.0): {len([s for s in scores if 1.0 <= s <= 2.0]):,}\")\n",
    "print(f\"  Mild (0.5-1.0): {len([s for s in scores if 0.5 <= s < 1.0]):,}\")\n",
    "print(f\"  Low/neutral (<0.5): {len([s for s in scores if s < 0.5]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Examples by Target Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMPLES BY TARGET GROUP\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXAMPLES BY TARGET GROUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check which target groups have the most examples\n",
    "target_counts = {}\n",
    "for col in target_cols:\n",
    "    count = len([ex for ex in data if ex.get(col) == True])\n",
    "    if count > 0:\n",
    "        target_counts[col.replace('target_', '')] = count\n",
    "\n",
    "# Sort by count\n",
    "sorted_targets = sorted(target_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 15 target groups by example count:\")\n",
    "for group, count in sorted_targets[:15]:\n",
    "    print(f\"  {group}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Examples for Each Placeholder Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PLACEHOLDER EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. [slur] - High insult + high hate speech score\n",
    "print(\"\\n1. [slur] - Looking for high insult scores...\")\n",
    "slurs = data.filter(lambda x: x['insult'] >= 4 and x['hate_speech_score'] > 2.0)\n",
    "print(f\"   Found {len(slurs):,} examples with high insult scores\")\n",
    "if len(slurs) > 0:\n",
    "    print(f\"   Example: {slurs[0]['text'][:150]}...\")\n",
    "\n",
    "# 2. [stereotype] - Moderate hate speech\n",
    "print(\"\\n2. [stereotype] - Looking for moderate hate speech...\")\n",
    "stereotypes = data.filter(lambda x: 0.5 <= x['hate_speech_score'] <= 1.5)\n",
    "print(f\"   Found {len(stereotypes):,} examples with moderate scores\")\n",
    "if len(stereotypes) > 0:\n",
    "    print(f\"   Example: {stereotypes[0]['text'][:150]}...\")\n",
    "\n",
    "# 3. [ethnic group] - Race/ethnicity targets\n",
    "print(\"\\n3. [ethnic group] - Looking for race/ethnicity targets...\")\n",
    "race_cols = [col for col in target_cols if 'race' in col]\n",
    "ethnic = data.filter(lambda x: any(x.get(col) == True for col in race_cols))\n",
    "print(f\"   Found {len(ethnic):,} examples targeting race/ethnicity\")\n",
    "if len(ethnic) > 0:\n",
    "    # Show which race\n",
    "    ex = ethnic[0]\n",
    "    races = [col.replace('target_race_', '') for col in race_cols if ex.get(col) == True]\n",
    "    print(f\"   Example targeting {', '.join(races)}: {ex['text'][:150]}...\")\n",
    "\n",
    "# 4. [group] - Any target group\n",
    "print(\"\\n4. [group] - General target groups...\")\n",
    "has_target = data.filter(lambda x: any(x.get(col) == True for col in target_cols))\n",
    "print(f\"   Found {len(has_target):,} examples with any target group\")\n",
    "\n",
    "# 5. Dehumanizing language\n",
    "print(\"\\n5. Dehumanizing language...\")\n",
    "dehumanize = data.filter(lambda x: x['dehumanize'] >= 3)\n",
    "print(f\"   Found {len(dehumanize):,} examples with dehumanizing language\")\n",
    "if len(dehumanize) > 0:\n",
    "    print(f\"   Example: {dehumanize[0]['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Filtering Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FILTERING EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example 1: Anti-Muslim hate speech\n",
    "print(\"\\nExample 1: Anti-Muslim content (high hate score)\")\n",
    "muslim_hate = data.filter(\n",
    "    lambda x: x.get('target_religion_muslim') == True and x['hate_speech_score'] > 1.5\n",
    ")\n",
    "print(f\"Found {len(muslim_hate)} examples\")\n",
    "if len(muslim_hate) > 0:\n",
    "    print(f\"Sample: {muslim_hate[0]['text'][:150]}...\")\n",
    "\n",
    "# Example 2: Anti-Black hate speech\n",
    "print(\"\\nExample 2: Anti-Black content (high hate score)\")\n",
    "black_hate = data.filter(\n",
    "    lambda x: x.get('target_race_black') == True and x['hate_speech_score'] > 1.5\n",
    ")\n",
    "print(f\"Found {len(black_hate)} examples\")\n",
    "if len(black_hate) > 0:\n",
    "    print(f\"Sample: {black_hate[0]['text'][:150]}...\")\n",
    "\n",
    "# Example 3: LGBTQ+ targeting\n",
    "print(\"\\nExample 3: LGBTQ+ targeting\")\n",
    "lgbtq_cols = [col for col in target_cols if 'sexuality' in col or 'transgender' in col]\n",
    "lgbtq_hate = data.filter(\n",
    "    lambda x: any(x.get(col) == True for col in lgbtq_cols) and x['hate_speech_score'] > 1.0\n",
    ")\n",
    "print(f\"Found {len(lgbtq_hate)} examples\")\n",
    "if len(lgbtq_hate) > 0:\n",
    "    print(f\"Sample: {lgbtq_hate[0]['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY FOR SCRIPT DEVELOPMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = {\n",
    "    'dataset_name': 'ucberkeley-dlab/measuring-hate-speech',\n",
    "    'total_examples': len(data),\n",
    "    'columns': data.column_names,\n",
    "    'key_fields': {\n",
    "        'text_field': 'text',\n",
    "        'hate_score_field': 'hate_speech_score',\n",
    "        'insult_field': 'insult',\n",
    "        'dehumanize_field': 'dehumanize'\n",
    "    },\n",
    "    'target_group_columns': target_cols,\n",
    "    'filtering_strategies': {\n",
    "        '[slur]': 'insult >= 4 AND hate_speech_score > 2.0',\n",
    "        '[stereotype]': '0.5 <= hate_speech_score <= 1.5',\n",
    "        '[ethnic group]': 'target_race_* == True',\n",
    "        '[group]': 'any target_* == True',\n",
    "        'dehumanizing': 'dehumanize >= 3'\n",
    "    },\n",
    "    'example_counts': {\n",
    "        'high_hate': len([s for s in scores if s > 2.0]),\n",
    "        'moderate_hate': len([s for s in scores if 1.0 <= s <= 2.0]),\n",
    "        'with_target_group': len(has_target)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "# Save to file\n",
    "with open('hate_speech_schema.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Schema saved to hate_speech_schema.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Samples for Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RANDOM SAMPLES (Quality Check)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get 5 random high-hate examples\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "high_hate_list = [ex for ex in data if ex['hate_speech_score'] > 2.0]\n",
    "samples = random.sample(high_hate_list, min(5, len(high_hate_list)))\n",
    "\n",
    "for idx, ex in enumerate(samples, 1):\n",
    "    print(f\"\\nSample {idx}:\")\n",
    "    print(f\"  Score: {ex['hate_speech_score']:.2f}\")\n",
    "    print(f\"  Text: {ex['text'][:200]}...\" if len(ex['text']) > 200 else f\"  Text: {ex['text']}\")\n",
    "    targets = [col.replace('target_', '') for col in target_cols if ex.get(col) == True]\n",
    "    if targets:\n",
    "        print(f\"  Targets: {', '.join(targets)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
